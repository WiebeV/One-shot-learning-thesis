{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model, Model\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data path\n",
    "#### You can add multiple file extensions by extending the glob as shown below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = glob.glob(\"/home/deeplearning/blob_storage/Classifier1/Klaar MA 17, wobbler2x, leaflet2x, display2x, poster gecorrigeerd4x, main banner4x/*.jpg\")\n",
    "images_paths.extend(glob.glob(\"/home/deeplearning/blob_storage/Classifier1/Klaar MA 17 Ceiling banner/*.jpg\"))\n",
    "images_paths.extend(glob.glob(\"/home/deeplearning/blob_storage/Classifier1/Klaar MA17 Sensormatic wrap /*.jpg\"))\n",
    "txt_paths = glob.glob(\"/home/deeplearning/blob_storage/Classifier1/Klaar MA 17, wobbler2x, leaflet2x, display2x, poster gecorrigeerd4x, main banner4x/*.txt\")\n",
    "txt_paths.extend(glob.glob(\"/home/deeplearning/blob_storage/Classifier1/Klaar MA 17 Ceiling banner/*.txt\"))\n",
    "txt_paths.extend(glob.glob(\"/home/deeplearning/blob_storage/Classifier1/Klaar MA17 Sensormatic wrap /*.txt\"))\n",
    "                           \n",
    "file_paths = []\n",
    "for txt in txt_paths:\n",
    "    if os.path.getsize(txt) > 0:\n",
    "        file_paths.append(txt.split('.')[0]+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684\n"
     ]
    }
   ],
   "source": [
    "images_paths = file_paths\n",
    "print(len(images_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_sprite(data):\n",
    "    \"\"\"\n",
    "    Creates the sprite image along with any necessary padding\n",
    "    Source : https://github.com/tensorflow/tensorflow/issues/6322\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    \n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "\n",
    "def populate_img_arr_modified(images_paths, size=(100,100),should_preprocess= False):\n",
    "    \"\"\"\n",
    "    Get an array of images for a list of image paths\n",
    "    Args:\n",
    "        size: the size of image , in pixels \n",
    "        should_preprocess: if the images should be processed (according to InceptionV3 requirements)\n",
    "    Returns:\n",
    "        arr: An array of the loaded images\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    arr1 = []\n",
    "    arr2 = []\n",
    "    for i,img_path in enumerate(images_paths):\n",
    "        img = image.load_img(img_path, target_size=size)\n",
    "        x = image.img_to_array(img)\n",
    "        x /=255\n",
    "        x_ = np.zeros((128,128,3))\n",
    "        y = np.zeros((128,128,3))\n",
    "        #x = np.expand_dims(x,axis=0)\n",
    "        arr.append(x_)#np.array([x_,y,x]))\n",
    "        arr1.append(y)\n",
    "        arr2.append(x)\n",
    "    arr = np.array(arr)\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    if should_preprocess:\n",
    "        arr = preprocess_input(arr)\n",
    "    return [arr, arr1, arr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_sprite(data):\n",
    "    \"\"\"\n",
    "    Creates the sprite image along with any necessary padding\n",
    "    Source : https://github.com/tensorflow/tensorflow/issues/6322\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    \n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "\n",
    "def populate_img_arr(images_paths, size=(100,100),should_preprocess= False):\n",
    "    \"\"\"\n",
    "    Get an array of images for a list of image paths\n",
    "    Args:\n",
    "        size: the size of image , in pixels \n",
    "        should_preprocess: if the images should be processed (according to InceptionV3 requirements)\n",
    "    Returns:\n",
    "        arr: An array of the loaded images\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    for i,img_path in enumerate(images_paths):\n",
    "        img = image.load_img(img_path, target_size=size)\n",
    "        x = image.img_to_array(img)\n",
    "        arr.append(x)\n",
    "    arr = np.array(arr)\n",
    "    if should_preprocess:\n",
    "        arr = preprocess_input(arr)\n",
    "    return arr  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "### If you want to use another model, you can change it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "base_model (Model)              (None, 128)          2705373     input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,705,373\n",
      "Trainable params: 2,705,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# trained_weights = 'siamese_128_batch64_ep06-0.15.h5'\n",
    "# model = load_model('../siamese_own_implementation/trained_models/Klaar IN32-1 deel 1 wobbler2x, leaflet2x, display2x, poster4x/' + trained_weights, compile=False)\n",
    "# model.summary()\n",
    "\n",
    "trained_weights = 'full_siamese_128_lr_0.003_FC_marg1_batch64_50ep.h5'\n",
    "model = load_model('../siamese_own_implementation/trained_models/Klaar IN32-1 alles/' + trained_weights , compile=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'base_model_3/lambda_5/l2_normalize:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].get_output_at(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Layer base_model has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2d20b236b2a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/anaconda3/envs/one_shotpy35/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 813\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer base_model has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "new_model = Model(model.inputs, model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 215552 into shape (1546,21025)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad548a2ed046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulate_img_arr_modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshould_preprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1546\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./oss_data/ma17_only_test_fullmodel.bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#del img_arr,preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 215552 into shape (1546,21025)"
     ]
    }
   ],
   "source": [
    "img_arr = populate_img_arr_modified(images_paths,size=(128,128),should_preprocess=False)\n",
    "preds = model.predict(img_arr,batch_size=64)[2]\n",
    "preds.reshape(1546,21025).tofile(\"./oss_data/ma17_only_test_fullmodel.bytes\")\n",
    "#del img_arr,preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1684, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.tofile(\"./oss_data/ma17_only_test_fullmodel.bytes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.reshape(1546,21025).tofile(\"./oss_data/ma17_only_test_fullmodel.bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imgs = populate_img_arr(images_paths ,size=(32,32),should_preprocess=False)\n",
    "sprite = Image.fromarray(images_to_sprite(raw_imgs).astype(np.uint8))\n",
    "sprite.save('./oss_data/ma17_only_test_fullmodel_smallimg.png')\n",
    "#del raw_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#raw_imgs = populate_img_arr(images_paths ,size=(128,128),should_preprocess=False)\n",
    "oss_json = json.load(open('./oss_data/oss_demo_projector_config.json'))\n",
    "tensor_shape = [raw_imgs.shape[0], model.output_shape[1]]\n",
    "single_image_dim = [raw_imgs.shape[1], raw_imgs.shape[2]]\n",
    "\n",
    "json_to_append = {\"tensorName\": 'Final_visual',\n",
    "                  \"tensorShape\": tensor_shape,\n",
    "                  \"tensorPath\": \"./oss_data/\" + 'ma17_only_test.bytes',\n",
    "                  \"sprite\": {\"imagePath\": \"./oss_data/\" + 'ma17_only_test.png',\n",
    "                             \"singleImageDim\": single_image_dim}}\n",
    "oss_json['embeddings'].append(json_to_append)\n",
    "with open('oss_data/oss_demo_projector_config.json', 'w+') as f:\n",
    "    json.dump(oss_json, f, ensure_ascii=False, indent=4)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python oneshot 35",
   "language": "python",
   "name": "one_shotpy35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
